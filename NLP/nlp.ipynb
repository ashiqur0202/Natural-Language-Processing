{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basics of NLP:\n",
    "   - Introduction to NLP and its applications\n",
    "   - Key concepts:\n",
    "     - Tokenization: Breaking text into smaller units (e.g., words, phrases)\n",
    "     - Lemmatization: Reducing words to their base or root forms\n",
    "     - Stemming: Reducing words to their stem or root form\n",
    "     - Stop words: Common words (e.g., \"the,\" \"a\") often removed during preprocessing\n",
    "     - Part-of-speech tagging: Assigning grammatical categories to words (e.g., noun, verb)\n",
    "     - Named entity recognition (NER): Identifying and classifying entities in text (e.g., names of people, organizations, locations)\n",
    "     - Syntactic parsing: Analyzing the grammatical structure of sentences\n",
    "     - Semantic analysis: Understanding the meaning of text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text\n",
    "text = \"Natural language processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'that', 'focus', 'on', 'the', 'interaction', 'between', 'computer', 'and', 'human', 'through', 'natural', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(\"Lemmas:\", lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stems: ['natur', 'languag', 'process', '(', 'nlp', ')', 'is', 'a', 'subfield', 'of', 'artifici', 'intellig', 'that', 'focus', 'on', 'the', 'interact', 'between', 'comput', 'and', 'human', 'through', 'natur', 'languag', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(token) for token in tokens]\n",
    "print(\"Stems:\", stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered words: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'artificial', 'intelligence', 'focuses', 'interaction', 'computers', 'humans', 'natural', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stop word removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"Filtered words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part-of-speech: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('subfield', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('that', 'WDT'), ('focuses', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('interaction', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('humans', 'NNS'), ('through', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Part-of-speech tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"Part-of-speech:\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: NLP - Label: ORGANIZATION\n"
     ]
    }
   ],
   "source": [
    "# Named entity recognition\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "# Output the named entities\n",
    "for entity in named_entities:\n",
    "    if hasattr(entity, 'label'):\n",
    "        print(f\"Entity: {' '.join(word for word, tag in entity)} - Label: {entity.label()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing:\n",
    "   - Cleaning and normalizing text data: Removing punctuation, numbers, and special characters; lowercasing text\n",
    "   - Tokenization: Breaking text into tokens (e.g., words, phrases)\n",
    "   - Stop word removal: Removing common words (e.g., \"the,\" \"a\") that don't provide much meaning\n",
    "   - Stemming and lemmatization: Reducing words to their base or root forms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "text = \"I love to eat apples. They taste like candy. I bought 5 kg of apples yesterday.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I love to eat apples. They taste like candy. I bought 5 kg of apples yesterday.\n",
      "Cleaned and Normalized Text: i love to eat apples they taste like candy i bought  kg of apples yesterday\n"
     ]
    }
   ],
   "source": [
    "def clean_and_normalize_text(text):\n",
    "    # Removing Punctuation, Numbers, and Special Characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Expanding Contractions (optional, requires additional data or packages)\n",
    "    # text = expand_contractions(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Clean and normalize text\n",
    "cleaned_text = clean_and_normalize_text(text)\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Text:\", text)\n",
    "print(\"Cleaned and Normalized Text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['i', 'love', 'to', 'eat', 'apples', 'they', 'taste', 'like', 'candy', 'i', 'bought', 'kg', 'of', 'apples', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    # Word Tokenization\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Sentence Tokenization (optional)\n",
    "    # sentences = sent_tokenize(text)\n",
    "\n",
    "    return words\n",
    "\n",
    "# Tokenize text\n",
    "tokens = tokenize_text(cleaned_text)\n",
    "\n",
    "# Print the results\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without Stopwords: ['love', 'eat', 'apples', 'taste', 'like', 'candy', 'bought', 'kg', 'apples', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(words):\n",
    "    # Removing Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "# Remove stopwords\n",
    "tokens_without_stopwords = remove_stopwords(tokens)\n",
    "\n",
    "# Print the results\n",
    "print(\"Tokens without Stopwords:\", tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['love', 'eat', 'appl', 'tast', 'like', 'candi', 'bought', 'kg', 'appl', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "def stem_words(words):\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return stemmed_words\n",
    "\n",
    "# Stem words\n",
    "stemmed_tokens = stem_words(tokens_without_stopwords)\n",
    "\n",
    "# Print the results\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['love', 'eat', 'appl', 'tast', 'like', 'candi', 'buy', 'kg', 'appl', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def lemmatize_words(words):\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "    return lemmatized_words\n",
    "\n",
    "# Lemmatize words\n",
    "lemmatized_tokens = lemmatize_words(stemmed_tokens)\n",
    "\n",
    "# Print the results\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text  \\\n",
      "0   1  I love to eat apples. They taste like candy. I...   \n",
      "1   2  Python is an awesome programming language. It ...   \n",
      "2   3  Today is a beautiful day. The weather is perfe...   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0  i love to eat apples they taste like candy i b...   \n",
      "1  python is an awesome programming language it i...   \n",
      "2  today is a beautiful day the weather is perfec...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [i, love, to, eat, apples, they, taste, like, ...   \n",
      "1  [python, is, an, awesome, programming, languag...   \n",
      "2  [today, is, a, beautiful, day, the, weather, i...   \n",
      "\n",
      "                            tokens_without_stopwords  \\\n",
      "0  [love, eat, apples, taste, like, candy, bought...   \n",
      "1  [python, awesome, programming, language, widel...   \n",
      "2  [today, beautiful, day, weather, perfect, picn...   \n",
      "\n",
      "                                      stemmed_tokens  \\\n",
      "0  [love, eat, appl, tast, like, candi, bought, k...   \n",
      "1  [python, awesom, program, languag, wide, use, ...   \n",
      "2  [today, beauti, day, weather, perfect, picnic,...   \n",
      "\n",
      "                                   lemmatized_tokens  \n",
      "0  [love, eat, appl, tast, like, candi, buy, kg, ...  \n",
      "1  [python, awesom, program, languag, wide, use, ...  \n",
      "2  [today, beauti, day, weather, perfect, picnic,...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample dataset of text documents\n",
    "data = {\n",
    "    'id': [1, 2, 3],\n",
    "    'text': [\n",
    "        \"I love to eat apples. They taste like candy. I bought 5 kg of apples yesterday.\",\n",
    "        \"Python is an awesome programming language. It is widely used in data science.\",\n",
    "        \"Today is a beautiful day. The weather is perfect for a picnic in the park.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Apply text preprocessing techniques to the 'text' column\n",
    "df['cleaned_text'] = df['text'].apply(clean_and_normalize_text)\n",
    "df['tokens'] = df['cleaned_text'].apply(tokenize_text)\n",
    "df['tokens_without_stopwords'] = df['tokens'].apply(remove_stopwords)\n",
    "df['stemmed_tokens'] = df['tokens_without_stopwords'].apply(stem_words)\n",
    "df['lemmatized_tokens'] = df['stemmed_tokens'].apply(lemmatize_words)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_without_stopwords</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I love to eat apples. They taste like candy. I...</td>\n",
       "      <td>i love to eat apples they taste like candy i b...</td>\n",
       "      <td>[i, love, to, eat, apples, they, taste, like, ...</td>\n",
       "      <td>[love, eat, apples, taste, like, candy, bought...</td>\n",
       "      <td>[love, eat, appl, tast, like, candi, bought, k...</td>\n",
       "      <td>[love, eat, appl, tast, like, candi, buy, kg, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Python is an awesome programming language. It ...</td>\n",
       "      <td>python is an awesome programming language it i...</td>\n",
       "      <td>[python, is, an, awesome, programming, languag...</td>\n",
       "      <td>[python, awesome, programming, language, widel...</td>\n",
       "      <td>[python, awesom, program, languag, wide, use, ...</td>\n",
       "      <td>[python, awesom, program, languag, wide, use, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Today is a beautiful day. The weather is perfe...</td>\n",
       "      <td>today is a beautiful day the weather is perfec...</td>\n",
       "      <td>[today, is, a, beautiful, day, the, weather, i...</td>\n",
       "      <td>[today, beautiful, day, weather, perfect, picn...</td>\n",
       "      <td>[today, beauti, day, weather, perfect, picnic,...</td>\n",
       "      <td>[today, beauti, day, weather, perfect, picnic,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0   1  I love to eat apples. They taste like candy. I...   \n",
       "1   2  Python is an awesome programming language. It ...   \n",
       "2   3  Today is a beautiful day. The weather is perfe...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  i love to eat apples they taste like candy i b...   \n",
       "1  python is an awesome programming language it i...   \n",
       "2  today is a beautiful day the weather is perfec...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [i, love, to, eat, apples, they, taste, like, ...   \n",
       "1  [python, is, an, awesome, programming, languag...   \n",
       "2  [today, is, a, beautiful, day, the, weather, i...   \n",
       "\n",
       "                            tokens_without_stopwords  \\\n",
       "0  [love, eat, apples, taste, like, candy, bought...   \n",
       "1  [python, awesome, programming, language, widel...   \n",
       "2  [today, beautiful, day, weather, perfect, picn...   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  [love, eat, appl, tast, like, candi, bought, k...   \n",
       "1  [python, awesom, program, languag, wide, use, ...   \n",
       "2  [today, beauti, day, weather, perfect, picnic,...   \n",
       "\n",
       "                                   lemmatized_tokens  \n",
       "0  [love, eat, appl, tast, like, candi, buy, kg, ...  \n",
       "1  [python, awesom, program, languag, wide, use, ...  \n",
       "2  [today, beauti, day, weather, perfect, picnic,...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Representation:\n",
    "   - Bag of Words (BoW) model: Representing text as a collection of word counts\n",
    "   - TF-IDF (Term Frequency-Inverse Document Frequency): A numerical statistic that reflects the importance of a word in a document relative to a collection of documents\n",
    "   - Word embeddings: Representing words in a continuous vector space (e.g., Word2Vec, GloVe, BERT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "documents = [\"I love cats\", \"I love dogs\"]\n",
    "tokens = [word_tokenize(doc.lower()) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword Removal and Stemming/Lemmatization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tokens = []\n",
    "for doc_tokens in tokens:\n",
    "    filtered_tokens = [ps.stem(token) for token in doc_tokens if token not in stop_words]\n",
    "    cleaned_tokens.append(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) model:\n",
      "['cat' 'dog' 'love']\n",
      "[[1 0 1]\n",
      " [0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words (BoW) model\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform([' '.join(doc) for doc in cleaned_tokens])\n",
    "bow_features = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Bag of Words (BoW) model:\")\n",
    "print(bow_features)\n",
    "print(bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF (Term Frequency-Inverse Document Frequency):\n",
      "['cat' 'dog' 'love']\n",
      "[[0.81480247 0.         0.57973867]\n",
      " [0.         0.81480247 0.57973867]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([' '.join(doc) for doc in cleaned_tokens])\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"\\nTF-IDF (Term Frequency-Inverse Document Frequency):\")\n",
    "print(tfidf_features)\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Named Entity Recognition (NER):\n",
    "   - Identifying and classifying entities in text (e.g., names of people, organizations, locations)\n",
    "   - NER libraries and tools (e.g., spaCy, Stanford NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity: Apple, Type: GPE\n",
      "Named Entity: Cupertino, Type: GPE\n",
      "Named Entity: California, Type: GPE\n",
      "Named Entity: Steve Jobs, Type: PERSON\n",
      "Named Entity: Steve Wozniak, Type: PERSON\n",
      "Named Entity: Ronald Wayne, Type: PERSON\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Install NLTK\n",
    "# If you haven't installed NLTK, you can install it using pip:\n",
    "# pip install nltk\n",
    "\n",
    "# Step 2: Import the necessary libraries\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "\n",
    "# Step 3: Download the necessary NLTK resources (if not already downloaded)\n",
    "# You need to download the 'punkt' tokenizer and 'maxent_ne_chunker' for the Named Entity Recognition (NER) to work properly.\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Step 4: Define a sample text to be analyzed\n",
    "text = \"Apple is a technology company headquartered in Cupertino, California. It was founded in 1976 by Steve Jobs, Steve Wozniak, and Ronald Wayne.\"\n",
    "\n",
    "# Step 5: Tokenize the text into words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Step 6: Part-of-Speech (POS) tagging\n",
    "# POS tagging is a step that assigns a part of speech to each token (word) in the text (e.g., noun, verb, adjective).\n",
    "tagged_words = pos_tag(words)\n",
    "\n",
    "# Step 7: Apply Named Entity Recognition (NER)\n",
    "# NER is a step that identifies and classifies named entities (e.g., names of people, organizations, locations) in the text.\n",
    "named_entities = ne_chunk(tagged_words)\n",
    "\n",
    "# Step 8: Print the named entities\n",
    "# This step prints the identified named entities along with their types.\n",
    "for entity in named_entities:\n",
    "    if isinstance(entity, nltk.Tree):\n",
    "        print(f'Named Entity: {\" \".join([x[0] for x in entity])}, Type: {entity.label()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Classification:\n",
    "   - Supervised learning: Using labeled data to train models to classify text into predefined categories (e.g., sentiment analysis, spam detection)\n",
    "   - Algorithms: Naive Bayes, Support Vector Machines (SVM), Random Forest, Gradient Boosting, Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "data = [\n",
    "    ('I am happy', 'positive'),\n",
    "    ('I am sad', 'negative'),\n",
    "    ('I feel great', 'positive'),\n",
    "    ('I feel terrible', 'negative'),\n",
    "    ('I am not happy', 'negative'),\n",
    "    ('I am not sad', 'positive')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (X) and labels (y)\n",
    "X = [text for text, label in data]\n",
    "y = [label for text, label in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize, remove stopwords, and lemmatize each text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "X_processed = []\n",
    "\n",
    "for text in X:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    X_processed.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Train a classifier (Multinomial Naive Bayes)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00       1.0\n",
      "    positive       0.00      0.00      0.00       1.0\n",
      "\n",
      "    accuracy                           0.00       2.0\n",
      "   macro avg       0.00      0.00      0.00       2.0\n",
      "weighted avg       0.00      0.00      0.00       2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluate the classifier\n",
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Classification Report:\\n{report}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sentiment Analysis:\n",
    "   - Analyzing text to determine sentiment (positive, negative, neutral)\n",
    "   - Sentiment lexicons and dictionaries (e.g., VADER, SentiWordNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')  # Download the VADER lexicon for sentiment analysis\n",
    "nltk.download('movie_reviews')  # Download the movie reviews dataset for training a classifier (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.358, 'pos': 0.642, 'compound': 0.8745}\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create a SentimentIntensityAnalyzer object\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a sample text\n",
    "text = \"I love NLTK! It's the best library ever!\"\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sentiment = sia.polarity_scores(text)\n",
    "\n",
    "# Print the sentiment scores\n",
    "print(sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.358, 'pos': 0.642, 'compound': 0.8745}\n"
     ]
    }
   ],
   "source": [
    "# Custom Sentiment Analysis\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create a SentimentIntensityAnalyzer object\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a sample text\n",
    "text = \"I love NLTK! It's the best library ever!\"\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sentiment = sia.polarity_scores(text)\n",
    "\n",
    "# Print the sentiment scores\n",
    "print(sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.358, 'pos': 0.642, 'compound': 0.8745}\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis with Custom Text\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create a SentimentIntensityAnalyzer object\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a sample text\n",
    "text = \"I love NLTK! It's the best library ever!\"\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sentiment = sia.polarity_scores(text)\n",
    "\n",
    "# Print the sentiment scores\n",
    "print(sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.358, 'pos': 0.642, 'compound': 0.8745}\n",
      "{'neg': 0.0, 'neu': 0.328, 'pos': 0.672, 'compound': 0.6239}\n",
      "{'neg': 0.773, 'neu': 0.227, 'pos': 0.0, 'compound': -0.7783}\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis on Multiple Texts\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Create a SentimentIntensityAnalyzer object\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a list of sample texts\n",
    "texts = [\n",
    "    \"I love NLTK! It's the best library ever!\",\n",
    "    \"NLTK is amazing!\",\n",
    "    \"I hate NLTK. It's terrible.\"\n",
    "]\n",
    "\n",
    "# Perform sentiment analysis on each text\n",
    "for text in texts:\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    print(sentiment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Custom Sentiment Analysis with NLTK: using movie reviews dataset\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "\n",
    "# Get the movie reviews dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Define the featureset (bag of words)\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print('Accuracy:', accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Language Models and Text Generation:\n",
    "   - Pretrained language models (e.g., GPT, BERT, T5)\n",
    "   - Text generation using language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movie reviews and create labeled data\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load the text data\n",
    "nltk.download('gutenberg')\n",
    "moby_dick = gutenberg.raw('melville-moby_dick.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocess the text\n",
    "moby_dick = moby_dick.lower()  # Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into words\n",
    "words = nltk.word_tokenize(moby_dick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create the Language Model\n",
    "# Unigram Model\n",
    "unigrams = ngrams(words, 1)\n",
    "unigram_model = FreqDist(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Model\n",
    "bigrams = ngrams(words, 2)\n",
    "bigram_model = nltk.ConditionalFreqDist((prev_word, word) for prev_word, word in bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'max'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Generate text using the unigram model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43munigram_model\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Generate text using the bigram model\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(generate_text(bigram_model))\n",
      "Cell \u001b[1;32mIn[119], line 7\u001b[0m, in \u001b[0;36mgenerate_text\u001b[1;34m(model, num_words)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_words):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prev_word:\n\u001b[1;32m----> 7\u001b[0m         next_word \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprev_word\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m()  \u001b[38;5;66;03m# Choose the most frequent next word\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m         next_word \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mkeys()))  \u001b[38;5;66;03m# Choose a random starting word\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'max'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Generate Text\n",
    "def generate_text(model, num_words=100):\n",
    "    text = []\n",
    "    prev_word = None\n",
    "    for _ in range(num_words):\n",
    "        if prev_word:\n",
    "            next_word = model[prev_word].max()  # Choose the most frequent next word\n",
    "        else:\n",
    "            next_word = random.choice(list(model.keys()))  # Choose a random starting word\n",
    "        text.append(next_word)\n",
    "        prev_word = next_word\n",
    "    return ' '.join(text)\n",
    "\n",
    "# Generate text using the unigram model\n",
    "print(generate_text(unigram_model))\n",
    "\n",
    "# Generate text using the bigram model\n",
    "print(generate_text(bigram_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Machine Translation:\n",
    "   - Translating text from one language to another\n",
    "   - NLP libraries for machine translation (e.g., Google Translate API)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -> None\n",
      "am -> None\n",
      "learning -> None\n",
      "natural -> None\n",
      "language -> None\n",
      "processing -> None\n",
      ". -> None\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate import IBMModel1\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.api import AlignedSent\n",
    "from nltk.translate.api import Alignment\n",
    "\n",
    "english_sentence = \"I am learning natural language processing.\"\n",
    "\n",
    "# Tokenize the English sentence\n",
    "english_tokens = word_tokenize(english_sentence)\n",
    "\n",
    "# Create an AlignedSent instance with English and Bengali sentences\n",
    "aligned_sent = AlignedSent(english_tokens, [])\n",
    "\n",
    "# Create a list of AlignedSent instances\n",
    "aligned_corpus = [aligned_sent]\n",
    "\n",
    "# Create an IBMModel1 instance\n",
    "ibm_model1 = IBMModel1(aligned_corpus, 10)\n",
    "\n",
    "# Translate each English word to Bengali\n",
    "for english_word in english_tokens:\n",
    "    # Get the translation probabilities for the English word\n",
    "    probs = ibm_model1.translation_table[english_word]\n",
    "    \n",
    "    # Find the Bengali word with the highest probability\n",
    "    max_prob_word = max(probs, key=probs.get)\n",
    "    \n",
    "    # Print the translation\n",
    "    print(f\"{english_word} -> {max_prob_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IBMModel1' object has no attribute 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Translate each English word to Bengali\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m english_sentence, bengali_sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(english_tokenized_corpus, bengali_tokenized_corpus):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Find the Bengali translation for each word in the English sentence\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     bengali_translation \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mibm_model1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43menglish_sentence\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Print the English sentence and its Bengali translation\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(english_sentence)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[125], line 32\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Translate each English word to Bengali\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m english_sentence, bengali_sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(english_tokenized_corpus, bengali_tokenized_corpus):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# Find the Bengali translation for each word in the English sentence\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     bengali_translation \u001b[38;5;241m=\u001b[39m [\u001b[43mibm_model1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m english_sentence]\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Print the English sentence and its Bengali translation\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnglish: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(english_sentence)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'IBMModel1' object has no attribute 'translate'"
     ]
    }
   ],
   "source": [
    "from nltk.translate import IBMModel1\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.api import AlignedSent\n",
    "\n",
    "# Sample sentence-aligned corpus\n",
    "english_corpus = [\n",
    "    \"I am learning natural language processing.\",\n",
    "    \"He likes to play football.\",\n",
    "    \"She is a doctor.\"\n",
    "]\n",
    "bengali_corpus = [\n",
    "    \"আমি ন্যাচুরাল ভাষা প্রসেসিং শেখছি।\",\n",
    "    \"তার পছন্দ ফুটবল খেলা।\",\n",
    "    \"তিনি ডাক্তার।\"\n",
    "]\n",
    "\n",
    "# Tokenize the English sentences\n",
    "english_tokenized_corpus = [word_tokenize(sentence) for sentence in english_corpus]\n",
    "\n",
    "# Tokenize the Bengali sentences\n",
    "bengali_tokenized_corpus = [word_tokenize(sentence) for sentence in bengali_corpus]\n",
    "\n",
    "# Create AlignedSent instances for each sentence pair\n",
    "aligned_corpus = [AlignedSent(english_tokens, bengali_tokens) for english_tokens, bengali_tokens in zip(english_tokenized_corpus, bengali_tokenized_corpus)]\n",
    "\n",
    "# Train an IBMModel1 instance using the aligned corpus\n",
    "ibm_model1 = IBMModel1(aligned_corpus, 10)\n",
    "\n",
    "# Translate each English word to Bengali\n",
    "for english_sentence, bengali_sentence in zip(english_tokenized_corpus, bengali_tokenized_corpus):\n",
    "    # Find the Bengali translation for each word in the English sentence\n",
    "    bengali_translation = [ibm_model1.translate(word) for word in english_sentence]\n",
    "    \n",
    "    # Print the English sentence and its Bengali translation\n",
    "    print(f\"English: {' '.join(english_sentence)}\")\n",
    "    print(f\"Bengali: {' '.join(bengali_translation)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Question Answering Systems:\n",
    "   - Building systems that can answer questions posed in natural language\n",
    "   - Datasets and benchmarks (e.g., SQuAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
